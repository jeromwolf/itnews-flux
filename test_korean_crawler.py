#!/usr/bin/env python3
"""í•œêµ­ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸."""

import sys
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

from src.news.crawler import create_news_crawler


def test_korean_crawlers():
    """í•œêµ­ IT ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸."""
    print("=" * 70)
    print("í•œêµ­ IT ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 70)

    sources = ["etnews", "zdnet_kr"]

    for source in sources:
        print(f"\n{'='*70}")
        print(f"ğŸ“° {source.upper()} í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
        print(f"{'='*70}\n")

        try:
            # í¬ë¡¤ëŸ¬ ìƒì„±
            crawler = create_news_crawler(source)
            print(f"âœ… í¬ë¡¤ëŸ¬ ìƒì„± ì„±ê³µ: {crawler.source.display_name}")
            print(f"   RSS URL: {crawler.rss_url}\n")

            # ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            print("ğŸ“¥ ë‰´ìŠ¤ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
            news_collection = crawler.fetch_news(limit=5, max_age_hours=48)

            print(f"âœ… ë‰´ìŠ¤ {news_collection.total}ê°œ ê°€ì ¸ì˜´\n")

            # ë‰´ìŠ¤ ì¶œë ¥
            for i, news in enumerate(news_collection.articles, 1):
                print(f"{i}. {news.title}")
                print(f"   ğŸ“… {news.published_at.strftime('%Y-%m-%d %H:%M')}")
                print(f"   ğŸ”— {news.url}")
                print(f"   ğŸ“Š ì¹´í…Œê³ ë¦¬: {news.category.value}")
                print(f"   â­ ì¤‘ìš”ë„: {news.importance.value}")
                print(f"   ğŸ“ˆ ì ìˆ˜: {news.calculate_score():.2f}")
                if news.summary:
                    summary = news.summary[:100] + "..." if len(news.summary) > 100 else news.summary
                    print(f"   ğŸ“ ìš”ì•½: {summary}")
                print()

        except Exception as e:
            print(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            import traceback

            traceback.print_exc()

    print("\n" + "=" * 70)
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 70)


if __name__ == "__main__":
    test_korean_crawlers()
